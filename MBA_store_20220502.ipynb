{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "894f6b70-c700-4c43-9f0f-0013cbd3953b",
   "metadata": {},
   "source": [
    "# Market Basket Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b212fc9-1687-42db-a267-030b0afb3eef",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Market Basket Analysis (MBA) is a data mining technique that is used to uncover purchase patterns in any retail setting. <br>\n",
    "The goal of Market Basket Analysis is to understand consumer behavior by identifying relationships between the items that people buy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562bcb3b-09b2-446d-89d8-0b2dce61ba84",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93d2a7f5-6fd3-4c21-86e1-1765ae782272",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "pd.options.mode.chained_assignment = None\n",
    "import datetime as dt\n",
    "from math import sqrt\n",
    "from mlxtend.frequent_patterns import apriori, association_rules # analysis\n",
    "\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092909a3-0c74-4c31-a3a8-8f3930719e22",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Universe\n",
    "Definição do cenário base:\n",
    "\n",
    ">  * Análise de transações de todas as lojas LM\n",
    "\n",
    "> * Período tempo:\n",
    "    * 2021 <br>\n",
    "\n",
    ">* Considerar todas as transações (tickets) que contém pelo menos um artigo da secção 7 (Sanitário)\n",
    "\n",
    "> * Incidência da análise será sobre todo o tipo de clientes;\n",
    "\n",
    "\n",
    "> * Análise desenvolvida em diferentes granularidades para conseguirmos aferir a que produz insigths mais relevantes (Familia de produto, produto, etc …):\n",
    "\n",
    "\n",
    "> * Compras online e loja física;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7170336-1fab-4e49-9672-82b15fd3324f",
   "metadata": {},
   "source": [
    "## Variables Description\n",
    "\n",
    "Ticket information\n",
    "________________________\n",
    "    'DAT_VTE' - Purchase date \n",
    "    'NUM_TIC' - Ticket ID\n",
    "    'NUM_RGRPCLI' - Client ID\n",
    "    'LIB_TYPFID_CART' - Type of fidelity card\n",
    "    'NUM_PERSLM_ENT_CRE' - ID of who made the sale \n",
    "        (to distinguish between in-store and online purchase; equal to 99999999 or 999016 means online; otherwise in-store purchase)\n",
    "    'MNT_TTCDEVETT' - Amount spent\n",
    "\n",
    "Store\n",
    "________________________\n",
    "    'NUM_ETT' - Store ID\n",
    "    'LIB_MAG' - Store name\n",
    "    'NUM_REG' - Store region ID\n",
    "    'LIB_REG' - Store region name\n",
    "    'NUM_RGRP' - Store format ID\n",
    "    'LIB_RGRP' - Store format name\n",
    "    \n",
    "Item\n",
    "________________________\n",
    "    'NUM_ART' - Item ID\n",
    "    'LIB_ART' - Item name\n",
    "    'NUM_RAY' - Section ID\n",
    "    'LIBNUMRAY' - Section name\n",
    "    'NUM_SRAY' - Subsection ID\n",
    "    'LIBCODSRAY' - Subsection name\n",
    "    'NUM_TYP' - Type ID\n",
    "    'LIBCODTYP' - Type name\n",
    "    'NUM_STYP' - Subtype ID\n",
    "    'LIBCODSTYP' - Subtype name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4747f58f-54dc-4c48-8657-35c7b25fefd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_null_table(data):\n",
    "    \"\"\" Return the information about missing values in data.\n",
    "    \n",
    "    \"\"\"\n",
    "    null_columns = (data.isnull().sum(axis = 0)/len(data)).sort_values(ascending=False).index\n",
    "    null_data = pd.concat([\n",
    "    data.isnull().sum(axis = 0),\n",
    "    (data.isnull().sum(axis = 0)/len(data)).sort_values(ascending=False),\n",
    "    data.loc[:, data.columns.isin(list(null_columns))].dtypes], axis=1)\n",
    "    null_data = null_data.rename(columns={0: '# null', \n",
    "                                          1: '% null', \n",
    "                                          2: 'type'}).sort_values(ascending=False, by = '% null')\n",
    "    null_data = null_data[null_data[\"# null\"]!=0]\n",
    "    \n",
    "    return null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d23e5daa-d255-4b0d-b08f-23c38ec6c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stores(bq_table):\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"\n",
    "        SELECT DISTINCT(NUM_ETT) \n",
    "        FROM `{}`\n",
    "        ORDER BY 1\n",
    "        \"\"\".format(bq_table)\n",
    "    query_job = client.query(query=query)\n",
    "    results = query_job.to_dataframe()\n",
    "\n",
    "    return list(results['NUM_ETT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b5fa934-bf4b-4085-917a-686d9abb84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(bq_table, store):\n",
    "    \"\"\" Return the respective sales dataset from stores.\n",
    "    \n",
    "    \"\"\"\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"\n",
    "                SELECT * \n",
    "                FROM `{}`\n",
    "                WHERE NUM_ETT = {}\n",
    "                \"\"\".format(bq_table,store)\n",
    "    query_job = client.query(query=query)\n",
    "    dataset = query_job.to_dataframe()\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae1351e8-0a02-41fa-9683-4ff3010698f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_online(bq_table):\n",
    "    \"\"\" Return the respective sales dataset from stores.\n",
    "    \n",
    "    \"\"\"\n",
    "    client = bigquery.Client()\n",
    "    query = \"\"\"\n",
    "                SELECT * \n",
    "                FROM `{}`\n",
    "                WHERE NUM_PERSLM_ENT_CRE IN (99999999,999016)\n",
    "                \"\"\".format(bq_table)\n",
    "    query_job = client.query(query=query)\n",
    "    dataset = query_job.to_dataframe()\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fae3fe0-b9f8-4fa8-882c-b3743b420491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversions(data, var_type = []):\n",
    "    \"\"\" Converts all 'int64' variables to 'object' type and specified conversions. Returns dataset with converted variables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dataset\n",
    "        The input sales dataset.\n",
    "    var_type : list of lists with len 2\n",
    "        [variable, type] list.\n",
    "    \n",
    "    \"\"\"\n",
    "    cols_to_change = data.select_dtypes(include='int64').columns\n",
    "    data[cols_to_change] = data[cols_to_change].astype('object')\n",
    "\n",
    "    for var in var_type:\n",
    "        data[var[0]] = data[var[0]].astype(var[1])\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1827abcb-d701-428f-bbb5-8075cd351cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rows_by_products(data, col, products):\n",
    "    \"\"\" Return dataset without the specified products (e.g. plastic bags) \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dataset\n",
    "        The input sales dataset.\n",
    "    col : string\n",
    "        The column name for the products variable.\n",
    "    products : list\n",
    "        List of products to remove.\n",
    "    \n",
    "    \"\"\"\n",
    "    return data[~data[col].isin(products)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c171eec-9435-4b98-b1b6-7cebd28276ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    \"\"\" Basic DS preprocessing.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Variables definition\n",
    "    #global df_final_nodups\n",
    "    #global types_purchase\n",
    "    #global lib_regs\n",
    "    #global lib_rgrps\n",
    "    #global stores\n",
    "    \n",
    "    # Keep non-negative values (exclude returns)\n",
    "    data = data[data.MNT_TTCDEVETT >= 0]\n",
    "    \n",
    "    # Add variable (online vs in-store purchase)\n",
    "    data['TYPE_PURCHASE'] = np.where(\n",
    "        data['NUM_PERSLM_ENT_CRE'].isin([99999999, 999016]), 'ONLINE', 'IN-STORE') \n",
    "    \n",
    "    # Composing an identifier for transactions - by date, store, and ticket number\n",
    "    data.loc[:, 'DAT_VTE'] = data['DAT_VTE'].astype('str')\n",
    "    data.loc[:, 'NUM_TIC'] = data['NUM_TIC'].astype('str')\n",
    "    data.loc[:, 'NUM_ETT'] = data['NUM_ETT'].astype('str')\n",
    "    data.loc[:, 'ETT_DAT_TIC'] = data['NUM_ETT'] + '-' + data['DAT_VTE'] + ' - ' + data['NUM_TIC']\n",
    "    \n",
    "    # Select tickets with at least one product from section 7 (Bathroom)\n",
    "    list_tickets = data[data['NUM_RAY']==7]['ETT_DAT_TIC']\n",
    "    df_final = data[data['ETT_DAT_TIC'].isin(list_tickets)].reset_index(drop=True)\n",
    "    \n",
    "    # Drop duplicates\n",
    "    df_final_nodups = df_final.drop_duplicates(keep='first', subset=['DAT_VTE','NUM_TIC','NUM_RAY','NUM_SRAY','NUM_TYP','NUM_STYP','NUM_ART','LIB_ART'], inplace=False)\n",
    "    \n",
    "    # Create lists of purchase types, regions, and store types - these will be needed for running iterations in MBA\n",
    "    types_purchase = df_final_nodups['TYPE_PURCHASE'].unique()\n",
    "    lib_regs = df_final_nodups['LIB_REG'].unique()\n",
    "    lib_rgrps = df_final_nodups['LIB_RGRP'].unique()\n",
    "    stores = df_final_nodups['NUM_ETT'].unique()\n",
    "    \n",
    "    # Get subjects (transactions) and slices (type/subtype of item) \n",
    "    df_final_nodups.loc[:, 'QTY'] = 1\n",
    "    df_final_nodups.loc[:, 'QTY'] = df_final_nodups['QTY'].astype('uint8')\n",
    "    df_final_nodups.loc[:, 'NUM_TYP'] = df_final_nodups['NUM_TYP'].astype('str')\n",
    "    df_final_nodups.loc[:, 'NUM_STYP'] = df_final_nodups['NUM_STYP'].astype('str')\n",
    "    df_final_nodups.loc[:, 'NUM_RAY'] = df_final_nodups['NUM_RAY'].astype('str')\n",
    "    df_final_nodups.loc[:, 'NUM_SRAY'] = df_final_nodups['NUM_SRAY'].astype('str')\n",
    "    df_final_nodups.loc[:, 'DAT_VTE'] = df_final_nodups['DAT_VTE'].astype('str')\n",
    "    df_final_nodups.loc[:, 'NUM_TIC'] = df_final_nodups['NUM_TIC'].astype('str')\n",
    "    df_final_nodups.loc[:, 'NUM_ETT'] = df_final_nodups['NUM_ETT'].astype('str')\n",
    "    \n",
    "    # Add variables\n",
    "    df_final_nodups['Year'] = pd.DatetimeIndex(df_final_nodups['DAT_VTE']).year\n",
    "    \n",
    "    # Define the naming of each slice\n",
    "    df_final_nodups.loc[:, 'TYP'] = df_final_nodups['NUM_RAY'] + '/' + df_final_nodups['NUM_SRAY'] + '/' + df_final_nodups['NUM_TYP'] + ': ' + df_final_nodups['LIBCODTYP']\n",
    "    \n",
    "    # Define the naming of each transaction\n",
    "    df_final_nodups.loc[:, 'DAT_NUM_TIC'] = df_final_nodups['NUM_ETT'] + '-' + df_final_nodups['DAT_VTE'] + ' - ' + df_final_nodups['NUM_TIC']\n",
    "    \n",
    "    return df_final_nodups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49697f42-cadb-42cc-8550-836a5a8d5bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_mba(basket,type_purchase,store_number):\n",
    "\n",
    "    basket_slice = basket[(basket.TYPE_PURCHASE == type_purchase) & (basket.NUM_ETT == store_number)]\n",
    "\n",
    "    nr_transac_items = basket_slice.shape[0]\n",
    "    lib_reg = basket_slice['LIB_REG'].iloc[0]\n",
    "    lib_rgrp = basket_slice['LIB_RGRP'].iloc[0]\n",
    "    store_name = basket_slice['LIB_MAGCRT'].iloc[0]\n",
    "    year = basket_slice['Year'].iloc[0]\n",
    "\n",
    "    # printing number of items\n",
    "    print('Transaction items for',type_purchase,store_number,':',nr_transac_items)\n",
    "    \n",
    "    # only advance if number of items is larger than zero\n",
    "    if nr_transac_items > 0:\n",
    "        calc_support = 0.001\n",
    "        \n",
    "        print('Applying apriory algorithm with minimum support =', calc_support)\n",
    "        \n",
    "        # getting data ready for apriori - this will produce a matrix with t rows and p columns\n",
    "        # t is the number of subjects - transactions\n",
    "        # p is the number of slices - product type/subtype\n",
    "        basket_df_final_nodups = (basket_slice.groupby(['DAT_NUM_TIC', 'TYP'])['QTY'].max().unstack(fill_value=0).reset_index().set_index('DAT_NUM_TIC'))\n",
    "        \n",
    "        # getting t from the matrix\n",
    "        nr_transacs = basket_df_final_nodups.shape[0]\n",
    "        \n",
    "        # run the apriori algorithm, inputting the matrix, min support, and setting some basic parameters\n",
    "        df_final_frq_items = apriori(basket_df_final_nodups, min_support = calc_support , use_colnames = True, low_memory = True)\n",
    "        \n",
    "        # print the shape of the itemset dataframe\n",
    "        print('Shape after apriori for',type_purchase,store_number,':',df_final_frq_items.shape,'\\n')\n",
    "        \n",
    "        # only advance if number of itemsets is larger than zero\n",
    "        if df_final_frq_items.shape[0] > 0:\n",
    "            # run the association_rules algorithm to get the rules\n",
    "            # input the itemset dataframe obtained by the aprori algorithm, and set parameters for minimum *quality* of rules\n",
    "            rules = association_rules(df_final_frq_items, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "            rules = rules.sort_values(['confidence', 'lift'], ascending=[False, False])\n",
    "            \n",
    "            # post-algorithm filtering\n",
    "            rules_filtered = rules[(rules.confidence > 0.20) & (rules.conviction > 1)]\n",
    "\n",
    "            rules_filtered['transactions'] = rules_filtered['support']*nr_transacs\n",
    "            rules_filtered['transactions'] = rules_filtered['transactions'].astype('int')\n",
    "            rules_filtered['total_transactions'] = nr_transacs\n",
    "            rules_filtered['LIB_REG'] = lib_reg\n",
    "            rules_filtered['LIB_RGRP'] = lib_rgrp\n",
    "            rules_filtered['NUM_ETT'] = store_number\n",
    "            rules_filtered['LIB_MAGCRT'] = store_name\n",
    "            rules_filtered['year'] = year\n",
    "            rules_filtered.rename({'antecedent support': 'antecedent_support', 'consequent support': 'consequent_support'}, axis=1, inplace=True)\n",
    "\n",
    "            # these steps are necessary to avoid antecedent and consequent itemsets being represented in the csv as:\n",
    "            # {frozenset: slgaslkn}\n",
    "            rules_filtered['antecedents'] = rules_filtered['antecedents'].apply(lambda x: list(x)[0:]).astype(\"unicode\")\n",
    "            rules_filtered['consequents'] = rules_filtered['consequents'].apply(lambda x: list(x)[0:]).astype(\"unicode\")\n",
    "            \n",
    "            rules_filtered['transactions_antecedents'] = rules_filtered['antecedent_support']*nr_transacs\n",
    "            rules_filtered['transactions_antecedents'] = rules_filtered['transactions_antecedents'].astype('int')\n",
    "            rules_filtered['transactions_consequents'] = rules_filtered['consequent_support']*nr_transacs\n",
    "            rules_filtered['transactions_consequents'] = rules_filtered['transactions_consequents'].astype('int')\n",
    "\n",
    "            rules_filtered['antecedents_section'] = rules_filtered['antecedents'].str.findall(r\"(?<=')\\d+\").str.join('; ')            \n",
    "            rules_filtered['antecedents_subsection'] = rules_filtered['antecedents'].str.findall(r\"(?<=/)\\d+(?=/)\").str.join('; ')\n",
    "            rules_filtered['antecedents_type'] = rules_filtered['antecedents'].str.findall(r\"(?<=/)\\d+(?=:)\").str.join('; ')\n",
    "\n",
    "            shapebeforefilt = rules.shape\n",
    "            shapeafterfilt = rules_filtered.shape\n",
    "            \n",
    "            # filter rules containing at least one product of section 7\n",
    "            regex = \"\\'7\"\n",
    "            antecedents_7 = rules_filtered['antecedents']\n",
    "            df_antecedents = antecedents_7.str.contains(regex)\n",
    "            consequents_7 = rules_filtered['consequents']\n",
    "            df_consequents = consequents_7.str.contains(regex)\n",
    "            rules_final = rules_filtered[(df_antecedents) | (df_consequents)]\n",
    "            rules_filtered = rules_final\n",
    "            \n",
    "            rules_filtered['suppconf']=rules_filtered['support']*rules_filtered['confidence']\n",
    "            \n",
    "            columns_to_include = ['year','LIB_RGRP','LIB_REG','NUM_ETT','LIB_MAGCRT',\n",
    "                                  'antecedents_section','antecedents_subsection','antecedents_type','antecedents',\n",
    "                                  'consequents','transactions',\n",
    "                                  'confidence','antecedent_support','consequent_support',\n",
    "                                  'support','lift','conviction','suppconf','total_transactions',\n",
    "                                  'transactions_antecedents','transactions_consequents',]\n",
    "            \n",
    "            rules_filtered = rules_filtered[columns_to_include]\n",
    "            rules_filtered = rules_filtered.sort_values('suppconf', ascending=False)\n",
    "            \n",
    "            filename = 'TYP07_RULES_' + type_purchase + '_' + str(year)\n",
    "            \n",
    "            if store_number=='1':\n",
    "                # delete existing table and create a new one\n",
    "                save_files(rules_filtered,'data-test-lab-lmpt.BaseTimestampLmPt',filename,'WRITE_TRUNCATE')\n",
    "                \n",
    "                # create new notes csv file\n",
    "                csv_file_notas = open(filename + '_notas', 'w')\n",
    "            \n",
    "            else:\n",
    "                save_files(rules_filtered,'data-test-lab-lmpt.BaseTimestampLmPt',filename,'WRITE_APPEND')\n",
    "                \n",
    "                # append notes in csv file\n",
    "                csv_file_notas = open(filename + '_notas', 'a')\n",
    "\n",
    "            #filename = 'TYP07_RULES_' + type_purchase + '_STORE_' + store_number + '_' + str(year)\n",
    "            #rules_filtered.to_gbq('BaseTimestampLmPt.'+filename,'data-test-lab-lmpt',if_exists='replace')\n",
    "            #save_files(rules_filtered,'data-test-lab-lmpt.BaseTimestampLmPt',filename)\n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "            # write in __notas\n",
    "            csv_file_notas.write('Rules for \\n Purchase type: ' + type_purchase + '\\n Year: ' + str(year) + '\\n Region: ' + lib_reg + '\\n Store type: ' + lib_rgrp + '\\n Store number: ' + store_number  + '\\n Store name: ' + store_name)\n",
    "            csv_file_notas.write('\\n Total number of transactions: ' + str(nr_transacs))\n",
    "            csv_file_notas.write('\\n Transaction items: ' + str(nr_transac_items) + '\\n Applying apriori algorithm with minimum support = ' + str(calc_support) )\n",
    "            csv_file_notas.write('\\n Shape after apriori: ' + str(shapebeforefilt) + '\\n Shape of rules with metric conditions: ' + str(shapeafterfilt)\n",
    "                                + '\\n Shape of filtered rules: ' + str(rules_filtered.shape) +'\\n')\n",
    "            csv_file_notas.write('--------------------------------------------------------------')\n",
    "            csv_file_notas.write('\\n')\n",
    "\n",
    "            csv_file_notas.close()                      \n",
    "\n",
    "        else:\n",
    "            # transactions in the slice but no itemsets after apriori\n",
    "            print('No rules were calculated because the basket is empty')\n",
    "    else:\n",
    "        # no transactions in the slice\n",
    "        print('No transactions, nothing was done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee6be39d-d49b-4426-8aec-9cafc23518c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_mba2(basket, type_purchase, store_number=None, calc_support = 0.001):\n",
    "\n",
    "    if type_purchase=='ONLINE':\n",
    "        basket_slice = basket[(basket.TYPE_PURCHASE == type_purchase)]\n",
    "\n",
    "        nr_transac_items = basket_slice.shape[0]\n",
    "        year = basket_slice['Year'].iloc[0]\n",
    "    \n",
    "        # printing number of items\n",
    "        print('Transaction items for',type_purchase,':',nr_transac_items)\n",
    "        \n",
    "    else:\n",
    "        basket_slice = basket[(basket.TYPE_PURCHASE == type_purchase) & (basket.NUM_ETT == store_number)]\n",
    "\n",
    "        nr_transac_items = basket_slice.shape[0]\n",
    "        lib_reg = basket_slice['LIB_REG'].iloc[0]\n",
    "        lib_rgrp = basket_slice['LIB_RGRP'].iloc[0]\n",
    "        store_name = basket_slice['LIB_MAGCRT'].iloc[0]\n",
    "        year = basket_slice['Year'].iloc[0]\n",
    "    \n",
    "        # printing number of items\n",
    "        print('Transaction items for',type_purchase,store_number,':',nr_transac_items)\n",
    "    \n",
    "    # only advance if number of items is larger than zero\n",
    "    if nr_transac_items > 0:\n",
    "        \n",
    "        print('Applying apriori algorithm with minimum support =', calc_support)\n",
    "        \n",
    "        # getting data ready for apriori - this will produce a matrix with t rows and p columns\n",
    "        # t is the number of subjects - transactions\n",
    "        # p is the number of slices - product type/subtype\n",
    "        basket_df_final_nodups = (basket_slice.groupby(['DAT_NUM_TIC', 'TYP'])['QTY'].max().unstack(fill_value=0).reset_index().set_index('DAT_NUM_TIC'))\n",
    "        \n",
    "        # getting t from the matrix\n",
    "        nr_transacs = basket_df_final_nodups.shape[0]\n",
    "        \n",
    "        # run the apriori algorithm, inputting the matrix, min support, and setting some basic parameters\n",
    "        df_final_frq_items = apriori(basket_df_final_nodups, min_support = calc_support , use_colnames = True, low_memory = True)\n",
    "\n",
    "        # print the shape of the itemset dataframe\n",
    "        if type_purchase=='ONLINE':\n",
    "            print('Shape after apriori for',type_purchase,':',df_final_frq_items.shape,'\\n')\n",
    "        else:\n",
    "            print('Shape after apriori for',type_purchase,store_number,':',df_final_frq_items.shape,'\\n')\n",
    "        \n",
    "        # only advance if number of itemsets is larger than zero\n",
    "        if df_final_frq_items.shape[0] > 0:\n",
    "            # run the association_rules algorithm to get the rules\n",
    "            # input the itemset dataframe obtained by the aprori algorithm, and set parameters for minimum *quality* of rules\n",
    "            rules = association_rules(df_final_frq_items, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "            rules = rules.sort_values(['confidence', 'lift'], ascending=[False, False])\n",
    "            \n",
    "            # post-algorithm filtering\n",
    "            rules_filtered = rules[(rules.confidence > 0.20) & (rules.conviction > 1)]\n",
    "\n",
    "            rules_filtered['transactions'] = rules_filtered['support']*nr_transacs\n",
    "            rules_filtered['transactions'] = rules_filtered['transactions'].astype('int')\n",
    "            rules_filtered['total_transactions'] = nr_transacs\n",
    "            rules_filtered['year'] = year\n",
    "\n",
    "            if type_purchase!='ONLINE':\n",
    "                rules_filtered['LIB_REG'] = lib_reg\n",
    "                rules_filtered['LIB_RGRP'] = lib_rgrp\n",
    "                rules_filtered['NUM_ETT'] = store_number\n",
    "                rules_filtered['LIB_MAGCRT'] = store_name\n",
    "                \n",
    "            rules_filtered.rename({'antecedent support': 'antecedent_support', 'consequent support': 'consequent_support'}, axis=1, inplace=True)\n",
    "\n",
    "            # these steps are necessary to avoid antecedent and consequent itemsets being represented in the csv as:\n",
    "            # {frozenset: slgaslkn}\n",
    "            rules_filtered['antecedents'] = rules_filtered['antecedents'].apply(lambda x: list(x)[0:]).astype(\"unicode\")\n",
    "            rules_filtered['consequents'] = rules_filtered['consequents'].apply(lambda x: list(x)[0:]).astype(\"unicode\")\n",
    "            \n",
    "            rules_filtered['transactions_antecedents'] = rules_filtered['antecedent_support']*nr_transacs\n",
    "            rules_filtered['transactions_antecedents'] = rules_filtered['transactions_antecedents'].astype('int')\n",
    "            rules_filtered['transactions_consequents'] = rules_filtered['consequent_support']*nr_transacs\n",
    "            rules_filtered['transactions_consequents'] = rules_filtered['transactions_consequents'].astype('int')\n",
    "\n",
    "            rules_filtered['antecedents_section'] = rules_filtered['antecedents'].str.findall(r\"(?<=')\\d+\").str.join('; ')            \n",
    "            rules_filtered['antecedents_subsection'] = rules_filtered['antecedents'].str.findall(r\"(?<=/)\\d+(?=/)\").str.join('; ')\n",
    "            rules_filtered['antecedents_type'] = rules_filtered['antecedents'].str.findall(r\"(?<=/)\\d+(?=:)\").str.join('; ')\n",
    "\n",
    "            shapebeforefilt = rules.shape\n",
    "            shapeafterfilt = rules_filtered.shape\n",
    "            \n",
    "            # filter rules containing at least one product of section 7\n",
    "            regex = \"\\'7\"\n",
    "            antecedents_7 = rules_filtered['antecedents']\n",
    "            df_antecedents = antecedents_7.str.contains(regex)\n",
    "            consequents_7 = rules_filtered['consequents']\n",
    "            df_consequents = consequents_7.str.contains(regex)\n",
    "            rules_final = rules_filtered[(df_antecedents) | (df_consequents)]\n",
    "            rules_filtered = rules_final\n",
    "            \n",
    "            rules_filtered['suppconf']=rules_filtered['support']*rules_filtered['confidence']\n",
    "            \n",
    "            \n",
    "            if type_purchase=='ONLINE':\n",
    "                columns_to_include = ['year',\n",
    "                                  'antecedents_section','antecedents_subsection','antecedents_type','antecedents',\n",
    "                                  'consequents','transactions',\n",
    "                                  'confidence','antecedent_support','consequent_support',\n",
    "                                  'support','lift','conviction','suppconf','total_transactions',\n",
    "                                  'transactions_antecedents','transactions_consequents',]\n",
    "            \n",
    "                rules_filtered = rules_filtered[columns_to_include]\n",
    "                rules_filtered = rules_filtered.sort_values('suppconf', ascending=False)\n",
    "\n",
    "                filename = 'TYP07_RULES_' + type_purchase + '_' + str(year)\n",
    "\n",
    "                # delete existing table and create a new one\n",
    "                save_files(rules_filtered,'data-test-lab-lmpt.BaseTimestampLmPt',filename,'WRITE_TRUNCATE')\n",
    "\n",
    "                # create new notes csv file\n",
    "                csv_file_notas = open(filename + '_notas', 'w')\n",
    "\n",
    "                # write in __notas\n",
    "                csv_file_notas.write('Rules for \\n Purchase type: ' + type_purchase + '\\n Year: ' + str(year))\n",
    "                csv_file_notas.write('\\n Total number of transactions: ' + str(nr_transacs))\n",
    "                csv_file_notas.write('\\n Transaction items: ' + str(nr_transac_items) + '\\n Applying apriori algorithm with minimum support = ' + str(calc_support) )\n",
    "                csv_file_notas.write('\\n Shape after apriori: ' + str(shapebeforefilt) + '\\n Shape of rules with metric conditions: ' + str(shapeafterfilt)\n",
    "                                    + '\\n Shape of filtered rules: ' + str(rules_filtered.shape) +'\\n')\n",
    "                csv_file_notas.write('--------------------------------------------------------------')\n",
    "                csv_file_notas.write('\\n')\n",
    "\n",
    "                csv_file_notas.close()    \n",
    "                \n",
    "            else: # IN-STORE\n",
    "                columns_to_include = ['year','LIB_RGRP','LIB_REG','NUM_ETT','LIB_MAGCRT',\n",
    "                                      'antecedents_section','antecedents_subsection','antecedents_type','antecedents',\n",
    "                                      'consequents','transactions',\n",
    "                                      'confidence','antecedent_support','consequent_support',\n",
    "                                      'support','lift','conviction','suppconf','total_transactions',\n",
    "                                      'transactions_antecedents','transactions_consequents',]\n",
    "\n",
    "\n",
    "                rules_filtered = rules_filtered[columns_to_include]\n",
    "                rules_filtered = rules_filtered.sort_values('suppconf', ascending=False)\n",
    "\n",
    "                filename = 'TYP07_RULES_' + type_purchase + '_' + str(year)\n",
    "\n",
    "                if store_number=='1':\n",
    "                    # delete existing table and create a new one\n",
    "                    save_files(rules_filtered,'data-test-lab-lmpt.BaseTimestampLmPt',filename,'WRITE_TRUNCATE')\n",
    "\n",
    "                    # create new notes csv file\n",
    "                    csv_file_notas = open(filename + '_notas', 'w')\n",
    "\n",
    "                else:\n",
    "                    save_files(rules_filtered,'data-test-lab-lmpt.BaseTimestampLmPt',filename,'WRITE_APPEND')\n",
    "\n",
    "                    # append notes in csv file\n",
    "                    csv_file_notas = open(filename + '_notas', 'a')\n",
    "\n",
    "                # write in __notas\n",
    "                csv_file_notas.write('Rules for \\n Purchase type: ' + type_purchase + '\\n Year: ' + str(year) + '\\n Region: ' + lib_reg + '\\n Store type: ' + lib_rgrp + '\\n Store number: ' + store_number  + '\\n Store name: ' + store_name)\n",
    "                csv_file_notas.write('\\n Total number of transactions: ' + str(nr_transacs))\n",
    "                csv_file_notas.write('\\n Transaction items: ' + str(nr_transac_items) + '\\n Applying apriori algorithm with minimum support = ' + str(calc_support) )\n",
    "                csv_file_notas.write('\\n Shape after apriori: ' + str(shapebeforefilt) + '\\n Shape of rules with metric conditions: ' + str(shapeafterfilt)\n",
    "                                    + '\\n Shape of filtered rules: ' + str(rules_filtered.shape) +'\\n')\n",
    "                csv_file_notas.write('--------------------------------------------------------------')\n",
    "                csv_file_notas.write('\\n')\n",
    "\n",
    "                csv_file_notas.close()                      \n",
    "\n",
    "        else:\n",
    "            # transactions in the slice but no itemsets after apriori\n",
    "            print('No rules were calculated because the basket is empty')\n",
    "    else:\n",
    "        # no transactions in the slice\n",
    "        print('No transactions, nothing was done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96ecd619-dfdb-43b5-a80f-a9dd6245dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_files(input_dataframe, bq_dataset, bq_table, write_disposition='WRITE_TRUNCATE'):\n",
    "    \"\"\" Saves tables into BigQuery project.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dataframe : dataframe\n",
    "        The input dataframe.\n",
    "    bq_dataset : string\n",
    "        The Dataset ID from BigQuery.\n",
    "    bq_table: string\n",
    "        The name for the new table.\n",
    "    write_disposition : string\n",
    "        Specifies the action that occurs if destination table already exists.\n",
    "        The default value is WRITE_TRUNCATE.\n",
    "        Possible values:\n",
    "            WRITE_APPEND - If the table already exists, BigQuery appends the data to the table.\n",
    "            WRITE_EMPTY - If the table already exists and contains data, a ‘duplicate’ error is returned in the job result.\n",
    "            WRITE_TRUNCATE - If the table already exists, BigQuery overwrites the table data.\n",
    "            \n",
    "    \"\"\"\n",
    "    # Load client\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Write output \n",
    "    destination = bigquery.Dataset(bq_dataset).table(bq_table)\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition = write_disposition,\n",
    "    )\n",
    "    job = client.load_table_from_dataframe(\n",
    "            input_dataframe, \n",
    "            destination, \n",
    "            job_config=job_config\n",
    "    )  # Make an API request.\n",
    "    job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7a5a66a-792e-42c1-a28e-6375d4bfd33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(data, store_number=None): \n",
    "    #var, type tuple\n",
    "    vars_to_convert = [\n",
    "        ['DAT_VTE','datetime64[ns]'],\n",
    "        ['NUM_RGRPCLI','object'],\n",
    "        ['MNT_TTCDEVETT','float64']\n",
    "    ]\n",
    "    data = conversions(data, vars_to_convert)\n",
    "    \n",
    "    filter_products = ['SACO PLASTICO LEVE UNIDADE','SACO PAPEL UNI']\n",
    "    data = filter_rows_by_products(data, 'LIB_ART', filter_products)\n",
    "    \n",
    "    basket = preprocessing(data)\n",
    "\n",
    "    if store_number:\n",
    "        do_mba2(basket,'IN-STORE',str(store_number))\n",
    "    else:\n",
    "        #do_mba2(basket,'ONLINE',calc_support = 0.007)\n",
    "        do_mba2(basket,'ONLINE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2152d064-4d5c-4d26-9caa-0759ad1211c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    bq_table='data-test-lab-lmpt.BaseTimestampLmPt.mba_2021'\n",
    "    \n",
    "    #store_arr = [1,9] #1,9,59\n",
    "    store_arr = get_stores(bq_table)\n",
    "    \n",
    "    # MBA per store\n",
    "    for store in store_arr:\n",
    "        df = get_data(bq_table,store)    \n",
    "        setup(df,store)\n",
    "    \n",
    "    # MBA for online\n",
    "    df_online = get_data_online(bq_table)\n",
    "    setup(df_online)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2b1a198-2f09-4a76-89a7-a50c77738e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction items for IN-STORE 1 : 287090\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 1 : (2301, 2) \n",
      "\n",
      "Transaction items for IN-STORE 2 : 677041\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 2 : (3845, 2) \n",
      "\n",
      "Transaction items for IN-STORE 3 : 516685\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 3 : (3521, 2) \n",
      "\n",
      "Transaction items for IN-STORE 4 : 311577\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 4 : (3778, 2) \n",
      "\n",
      "Transaction items for IN-STORE 5 : 724364\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 5 : (4318, 2) \n",
      "\n",
      "Transaction items for IN-STORE 6 : 316860\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 6 : (2301, 2) \n",
      "\n",
      "Transaction items for IN-STORE 7 : 370181\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 7 : (3144, 2) \n",
      "\n",
      "Transaction items for IN-STORE 8 : 238409\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 8 : (2408, 2) \n",
      "\n",
      "Transaction items for IN-STORE 9 : 334904\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 9 : (2734, 2) \n",
      "\n",
      "Transaction items for IN-STORE 10 : 276608\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 10 : (1923, 2) \n",
      "\n",
      "Transaction items for IN-STORE 11 : 267762\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 11 : (2232, 2) \n",
      "\n",
      "Transaction items for IN-STORE 12 : 335712\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 12 : (4394, 2) \n",
      "\n",
      "Transaction items for IN-STORE 13 : 274065\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 13 : (2486, 2) \n",
      "\n",
      "Transaction items for IN-STORE 14 : 250568\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 14 : (2470, 2) \n",
      "\n",
      "Transaction items for IN-STORE 16 : 86198\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 16 : (3993, 2) \n",
      "\n",
      "Transaction items for IN-STORE 17 : 162183\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 17 : (3018, 2) \n",
      "\n",
      "Transaction items for IN-STORE 18 : 175277\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 18 : (2670, 2) \n",
      "\n",
      "Transaction items for IN-STORE 31 : 146720\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 31 : (2184, 2) \n",
      "\n",
      "Transaction items for IN-STORE 33 : 131592\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 33 : (1749, 2) \n",
      "\n",
      "Transaction items for IN-STORE 34 : 169519\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 34 : (2540, 2) \n",
      "\n",
      "Transaction items for IN-STORE 35 : 16764\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 35 : (1219, 2) \n",
      "\n",
      "Transaction items for IN-STORE 36 : 236023\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 36 : (2682, 2) \n",
      "\n",
      "Transaction items for IN-STORE 38 : 138093\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 38 : (2426, 2) \n",
      "\n",
      "Transaction items for IN-STORE 39 : 81344\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 39 : (1458, 2) \n",
      "\n",
      "Transaction items for IN-STORE 40 : 60244\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 40 : (1855, 2) \n",
      "\n",
      "Transaction items for IN-STORE 41 : 34867\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 41 : (1286, 2) \n",
      "\n",
      "Transaction items for IN-STORE 43 : 99578\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 43 : (1993, 2) \n",
      "\n",
      "Transaction items for IN-STORE 44 : 94537\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 44 : (1406, 2) \n",
      "\n",
      "Transaction items for IN-STORE 45 : 42737\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 45 : (1145, 2) \n",
      "\n",
      "Transaction items for IN-STORE 46 : 102008\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 46 : (2399, 2) \n",
      "\n",
      "Transaction items for IN-STORE 47 : 58966\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 47 : (1396, 2) \n",
      "\n",
      "Transaction items for IN-STORE 48 : 85477\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 48 : (1659, 2) \n",
      "\n",
      "Transaction items for IN-STORE 49 : 59649\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 49 : (1306, 2) \n",
      "\n",
      "Transaction items for IN-STORE 50 : 131466\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 50 : (2039, 2) \n",
      "\n",
      "Transaction items for IN-STORE 51 : 85379\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 51 : (1606, 2) \n",
      "\n",
      "Transaction items for IN-STORE 52 : 61306\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 52 : (1174, 2) \n",
      "\n",
      "Transaction items for IN-STORE 54 : 49665\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 54 : (1695, 2) \n",
      "\n",
      "Transaction items for IN-STORE 55 : 131492\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 55 : (1628, 2) \n",
      "\n",
      "Transaction items for IN-STORE 56 : 55229\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 56 : (2157, 2) \n",
      "\n",
      "Transaction items for IN-STORE 57 : 139006\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 57 : (2271, 2) \n",
      "\n",
      "Transaction items for IN-STORE 58 : 51009\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 58 : (1784, 2) \n",
      "\n",
      "Transaction items for IN-STORE 59 : 40705\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 59 : (1659, 2) \n",
      "\n",
      "Transaction items for IN-STORE 60 : 138671\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 60 : (2191, 2) \n",
      "\n",
      "Transaction items for IN-STORE 61 : 52016\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 61 : (1323, 2) \n",
      "\n",
      "Transaction items for IN-STORE 62 : 27487\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 62 : (823, 2) \n",
      "\n",
      "Transaction items for IN-STORE 63 : 45821\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 63 : (2067, 2) \n",
      "\n",
      "Transaction items for IN-STORE 65 : 47668\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 65 : (1592, 2) \n",
      "\n",
      "Transaction items for IN-STORE 66 : 75705\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 66 : (1554, 2) \n",
      "\n",
      "Transaction items for IN-STORE 67 : 36360\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 67 : (1949, 2) \n",
      "\n",
      "Transaction items for IN-STORE 68 : 4152\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 68 : (1872, 2) \n",
      "\n",
      "Transaction items for IN-STORE 69 : 35784\n",
      "Applying apriori algorithm with minimum support = 0.001\n",
      "Shape after apriori for IN-STORE 69 : (1495, 2) \n",
      "\n",
      "Transaction items for ONLINE : 96037\n",
      "Applying apriori algorithm with minimum support = 0.007\n",
      "Shape after apriori for ONLINE : (71, 2) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "689e7fc8-640c-40b3-9c53-09cd30addcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all files\n",
    "#!zip -r MBA_20220502_allfiles.zip *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac81e90-ef4a-4bb6-8c72-e66e03f87ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
